Supplementary materials for “From measurement to understanding: an architecture of emulated expertise for LLM-diagnostics in education”

Authors: O.E. Klepikov, E.M. Pozdnyakova, Y.O. Sizov


General introduction
These supplementary materials consolidate the conceptual architecture, methodological rationale, and evaluative procedures of the system with sufficient depth to support scientific scrutiny, contextualization of results, and independent assessment of the claims made in the main text. The goal is to characterize the functional roles and interactions of the system’s cognitive archetypes and to document the empirical procedures used to generate and interpret the reported findings.
Scope. This document is limited to conceptual design, architectural patterns, decision rationales, and evaluation methodology. Environment-specific implementation assets - for example, production prompt libraries, agent- or deployment-specific orchestration logic, execution bindings, monitoring and audit checklists, runtime configurations, and other operational materials - are implementation-dependent and fall outside the scope of the present supplementary materials. The intent is to make the scientific basis of the work inspectable without extending into environment-specific deployment artifacts.
This document is therefore structured into a series of sequential, interdependent sections, each designed to build upon the last and provide a complete, multi-layered account of the project’s design.
●	Section S0: Methodology for translating psychometric instruments into behavioral analysis protocols. This section, which addresses a critical methodological gap, details the core, five-phase R&D process for translating any introspective psychometric instrument into a valid, text-based behavioral analysis protocol. It is this foundational methodology that ensures the "methodological grounding" claimed in the main article.
●	Section S1: Microarchitecture of the ‘prompt researcher’. This section provides a complete specification of the ‘researcher’ archetype, implemented in the domain expert pipeline. It moves beyond the heuristic core described in the main article to include the sub-processes for reflective self-audit (the confidence score calculation) and for the finalization and consistent quantification of knowledge.
●	Section S2: Microarchitecture of the ‘prompt auditor’. This section details the ‘auditor’ archetype, which is the architectural implementation of the principle of echeloned quality control. It describes the multi-tiered verification pipeline of the supervisor unit, from the audit of the evidence base to the final arbitration.
●	Section S3: Microarchitecture of the ‘prompt communicator’. This section describes the ‘communicator’ archetype, implemented in the simplifier pipeline. It outlines the multi-stage process for the ethical and didactic adaptation of the professional report for the end-user, including mechanisms for ensuring stylistic consistency and the dynamic generation of auxiliary materials.
●	Section S4: Multi-level validation protocol. This section presents the formal, detailed plan for the future empirical verification of the system, including its psychometric, didactic, and bias audit components.
●	Section S5: Epistemological and ontological foundations. This section elaborates on the deeper philosophical and theoretical grounding of the project in process-relational ontology and critical fallibilism.
●	Section S6: Full system blueprint and technical specifications. This section provides the formal reference and access links to the complete technical specification of the system, including agent passports and prompt templates, hosted in an external open-science repository.
Collectively, these sections are intended to provide a comprehensive, transparent, and reproducible account of the research, enabling a thorough evaluation of its scientific and engineering contributions.

Section S0: Methodology for translating psychometric instruments into behavioral analysis protocols
S0.1. Introduction and rationale
This section outlines the systematic, five-phase research and development (R&D) methodology created to translate established, self-report-based psychometric instruments into objective, text-based behavioral analysis protocols. The primary objective of this “translation” process is to shift the object of measurement from an individual’s self-perception to the observable, behavioral manifestations of their cognitive style and personality traits as they appear in naturally generated text. This approach is designed to bypass the inherent limitations of self-report methods, such as social desirability bias and insufficient self-reflection.
The methodology provides a structured, reproducible, and scientifically-grounded pathway for developing the external, explicit protocols that are executed by the LLM agents within the emulated expertise architecture. It is this process that ensures the “methodological grounding” and construct validity central to the main article’s thesis. The successful completion of these five phases results in a complete diagnostic module, comprising a detailed analytical framework, a formal coding protocol (‘codebook’), and a calibrated scoring formula.
S0.2. The five-phase development process
Phase I: Construct deconstruction and operationalization
The initial phase is a foundational theoretical analysis aimed at establishing a valid conceptual bridge between an abstract psychological construct and its objective, observable manifestations in text.
●	Step 1.1: Analysis of the source psychometric instrument. The process begins with a deep analysis of the original introspective instrument (the “gold standard”). This involves studying not only the item content but also its theoretical basis, factor structure, and psychometric properties. Items with the highest factor loadings are given special attention as they represent the core of the construct as conceptualized by its original author.
●	Step 1.2: Expanded theoretical investigation of the construct. The target construct is situated within a broader scientific context through a systematic literature review. Its established correlations with fundamental personality models (e.g., big five), cognitive styles, motivational theories, and other relevant psychological domains are examined. This step prevents a tautological definition (“the construct is what the test measures”) and ensures that the object of analysis is a stable, theoretically significant, and valid psychological reality.
●	Step 1.3: Strategic focus on a single aspect. Many psychological constructs are multidimensional. Attempting to operationalize a complex, multi-faceted construct in its entirety risks creating an unwieldy and internally inconsistent analytical tool. Therefore, a single, core aspect of the construct is selected for translation, ensuring a focused and methodologically tractable scope for each diagnostic module.
●	Step 1.4: Formulation of hypotheses about textual manifestations. A system of theoretically grounded hypotheses is developed, describing which specific, observable, and measurable patterns of textual behavior could serve as reliable indicators of the targeted psychological aspect. This process moves from the abstract to the concrete: the formal definition of the aspect (e.g., “cognitive flexibility”) is broken down into its constituent cognitive processes or behavioral tendencies (e.g., “the ability to generate multiple alternatives”), for which specific, testable hypotheses about their textual expression are formulated (e.g., a higher frequency of subjunctive mood, modal verbs of possibility, and counterfactual constructions).
Phase II: Development of the multi-level analytical framework
This phase transforms the hypotheses from phase I into a structured analytical matrix by selecting a validated set of analytical tools (“theoretical lenses”).
●	Step 2.1: Formation of a multi-level model of text analysis. A four-level hierarchical model is used to deconstruct the text into a multi-dimensional object for analysis, ensuring comprehensive data coverage. The model, adhering to the MECE principle (mutually exclusive, collectively exhaustive), consists of:
o	Level 1: Micro-level (sub-utterance units: graphical, morphological, lexico-semantic).
o	Level 2: Meso-level (utterance in local context: syntactic, semantic-propositional, pragmatic-discursive).
o	Level 3: Macro-level (text as a global whole: compositional, thematic, narrative, argumentative, stylistic).
o	Level 4: Meta-level (text in interpretive context: biographical, psychological, axiological, hermeneutic).
●	Step 2.2: Nomenclatural screening and selection of theoretical lenses. A systematic search is conducted across relevant academic disciplines (e.g., cognitive psychology, linguistics, rhetoric, semiotics) to identify a pool of potentially relevant theoretical frameworks, models, and methods (“lenses”). Each potential lens is subjected to a rigorous five-filter selection process:
o	Theoretical relevance: The lens must be designed to analyze the specific textual phenomenon identified in the hypotheses.
o	Operational applicability: The lens’s concepts must be translatable into concrete, observable, and measurable features in the text.
o	Empirical grounding and scientific status: Only lenses from recognized, peer-reviewed academic traditions are considered.
o	Inferential validity: The logical chain connecting the observable textual feature to the unobservable psychological construct must be short, direct, and robust against alternative explanations.
o	Reproducibility and reliability: Preference is given to lenses that support quantitative metrics or formalized analytical procedures, minimizing subjective judgment.
Phase III: Critical verification and hierarchization of the framework
This phase refines the broad toolkit from phase II into a prioritized, weighted, and methodologically defensible system.
●	Step 3.1: Hierarchization of analytical tools. All selected lenses, sub-levels, and levels are ranked according to their diagnostic value for the specific construct. The ranking is based on an integrated assessment of the strength of the inferential link, the reliability of the measurement, and the richness of the diagnostic information provided.
●	Step 3.2: Filtration and formation of the final analytical toolkit. Based on the hierarchy, a final, uncompromising filtration is performed. All lenses, sub-levels, or even entire levels deemed to have high inferential distance, insurmountable subjectivity, or low diagnostic value are definitively excluded from the final protocol.
●	Step 3.3: Formation of a system of meta-variables. A conceptual synthesis is performed to identify high-level, cross-cutting psychological or cognitive dimensions that manifest across multiple levels of analysis. These “meta-variables” (e.g., agency & locus of control, affective valence of risk) aggregate atomic linguistic features into psychologically meaningful clusters, providing a language for the final qualitative interpretation.
Phase IV: Methodological core: development of the analysis and assessment protocol
This phase operationalizes the filtered framework into a final, executable ‘codebook’ for the LLM agent.
●	Step 4.1: Exploratory analysis protocol. For each lens, a detailed search protocol is developed. This includes a formal list of specific, observable diagnostic features to be found in the text, and the logic of identification-a step-by-step algorithm or set of heuristics that guides the agent in systematically scanning the text to find evidence for each feature.
●	Step 4.2: Explicative analysis protocol. For each diagnostic feature, a corresponding interpretive protocol is developed. This includes:
o	The principle of inference: A formalized logical argument explaining why the presence of the observable textual feature allows for a valid inference about the unobservable psychological construct.
o	The logic of diagnostic interpretation: A qualitative description of what the presence, absence, or high degree of expression of the feature means, including the integration of falsifying hypotheses and conditions under which the feature should not be considered a valid indicator.
●	Step 4.3: Development of the scoring scale and coding protocol. A standardized 10-point ordinal scale (grouped into five conceptual levels: 0, 1-3, 4-6, 7-9, 10) is established. For each diagnostic feature, detailed qualitative descriptors (rubrics) are written for each level of the scale, translating the logic of interpretation into a concrete scoring guide. This culminates in the final ‘codebook’.
Phase V: Construction and calibration of the diagnostic formula
The final phase creates a simple, transparent mathematical model to aggregate the granular scores into a final, interpretable result.
●	Step 5.1: Definition of the formula’s architecture and principles. The formula is designed based on strict principles: mathematical simplicity (using the arithmetic mean), inclusivity (all validated features are included with equal weight), absence of complex modifiers, consistent positive valence for all variables, and metric comparability with the original instrument.
●	Step 5.2: Development of the scaling coefficient. To ensure the final score is metrically comparable to the scale of the original psychometric instrument, a linear transformation is applied.
●	Step 5.3: Formalization of the final equation. The entire calculation logic is formalized into a single, unambiguous mathematical expression. This final equation completes the diagnostic module.

Section S1: Microarchitecture of the ‘prompt researcher’
S1.1. Introduction
This section outlines the reference microarchitecture for the ‘prompt researcher’ archetype, describing the intended functional roles, information flow, and interaction patterns among subcomponents. The description is intended to clarify how the approach is organized conceptually and analytically so that its behavior and design motivations can be examined and discussed.
S1.2. Sub-process I: Hypothesis generation and verification (the heuristic core)
This sub-process is the heuristic core of the entire diagnostic system. Its goal is not simply to describe data, but to construct and critically test several competing, deep explanatory models (hypotheses) that reveal the internal structure and dynamics of the user’s personality. This process is implemented as a complex pipeline that emulates the dialectical method of scientific knowledge, using three key techniques.
●	S1.2.1. Hierarchical synthesis for complexity management. The system avoids the methodologically risky direct leap from “raw” data to global conclusions. Instead, it uses a two-step synthesis process that manages cognitive complexity and increases the reliability of analysis, consistent with hierarchical models of information processing in cognitive psychology (Craik & Lockhart, 1972). First, ‘agent-interpreters’ analyze the evidence base related to only one local aspect of the construct. Second, ‘agent-theorists’ receive these pre-processed, meaningful aspectual conclusions and integrate them into a global hypothesis across the entire construct.
●	S1.2.2. Perspective shifting as a driver of falsification. The mechanism of systematic falsification is implemented through an architectural coercion to change the analytical perspective. The pipeline sequentially activates three theoretical agents, each of which receives a different “setting,” implementing the dialectical triad of “thesis-antithesis-synthesis.” The first agent-theorist (‘confirmer’) formulates the thesis. The second agent-theorist (‘skeptic’) formulates an antithesis based on “inconvenient facts.” The third agent-theorist (‘dialectician’) formulates a synthesis-a more complex, integrating model that attempts to reconcile contradictions. This dialectical process, grounded in critical rationalism (Popper, 1959) and the multiple working hypotheses method (Chamberlin, 1890), ensures that the system does not stop at the first obvious conclusion.
●	S1.2.3. Formalized arbitration for objectifying choice. The final choice of the “winning” hypothesis is delegated to a separate, specialized ‘arbitrator agent’. This agent works in a strictly algorithmic mode, implementing a formal protocol based on inference to the best explanation. The protocol includes a validity filter (checking for consistency with all facts), a competitive comparison (evaluation based on explicit criteria like explanatory power and parsimony), and a final, reasoned verdict. This mechanism ensures that the final conclusion is not merely the first one that comes to mind, but the most robust and thoroughly tested.
S1.3. Sub-process II: Reflective self-audit (quantitative assessment of reliability)
After the system has generated and verified knowledge by selecting the “winning” hypothesis, it launches an internal cycle of epistemic reflection. The goal is not to obtain a new conclusion, but to quantitatively assess the reliability of the work already done. This process is implemented through the calculation of the ‘confidence score’ integral metric, which operationalizes metacognition-“thinking about thinking” (Flavell, 1979). The task is decomposed into a sub-pipeline of three specialized agents.
●	S1.3.1. Data audit. The ‘evidence base audit agent’ evaluates the completeness of the evidence base by calculating the ratio of criteria “covered” by textual evidence to their total number. This factor corresponds to the epistemic criterion of empirical adequacy.
●	S1.3.2. Argumentation audit. The ‘qualitative argument extractor agent’ and a subsequent ‘calculator agent’ evaluate three aspects of the argumentation that led to the winning hypothesis:
1.	Structural integrity: Analyzes the presence of synergies and the absence of unresolved conflicts in the final conclusion, corresponding to the criterion of internal coherence.
2.	Resistance to criticism: Analyzes the arguments “against” the winning hypothesis, assessing its robustness. This factor corresponds to Popper’s criterion of falsifiability.
3.	Diagnostic complexity: Analyzes the arguments “for” the second-place hypothesis, assessing the system’s explanatory power in ambiguous cases.
●	S1.3.3. Metric synthesis. The final ‘calculator agent’ applies a weighted formula to the assessments of all four factors, ensuring transparency and reproducibility of the calculation. This sub-process transforms the confidence score into meta-knowledge-knowledge about the quality of other knowledge-providing a transparent and justified assessment of the diagnostic conclusion’s reliability.
S1.4. Sub-process III: Finalization and quantification (the transition from quality to quantity)
This final sub-process solves the task of “packaging” the verified and evaluated knowledge into its final products. It is implemented as two parallel streams that transform the abstract “winning hypothesis” into a qualitative text report and a set of quantitative scores, while architecturally ensuring their mutual consistency.
●	S1.4.1. Qualitative finalization stream: rational reconstruction. This stream emulates the process of writing a scientific article after completing research, implementing the transition from the “context of discovery” to the “context of justification” (Reichenbach, 1938). An ‘agent-structurizer’ transforms the holistic hypothesis into a logical “skeleton” of a report with clearly defined semantic blocks. Then, an ‘agent-copywriter’ “fills” this skeleton with coherent, consistent, and stylistically accurate text, creating the final professional report.
●	S1.4.2. Quantitative finalization stream: consistent quantification. This stream solves the methodologically complex task of transitioning from idiographic understanding to nomothetic measurement. The ‘agent-evaluator’ implements a key methodological innovation. It uses a rubric-based evaluation technique, but with a critically important additional context-it is fed not only with evidence (quotes) and the scoring rubric, but also with the final qualitative conclusion. This architectural link forces the agent to assign scores that are consistent with the text conclusion already made, preventing inconsistencies between qualitative and quantitative assessments. This approach is an implementation of the principle of integration from mixed-methods research (Creswell & Plano Clark, 2017). Subsequent ‘calculator’ and ‘collector’ agents then perform the technical functions of aggregating these detailed assessments into final scores. This dual finalization process ensures that the system produces not only in-depth (qualitative) and comparable (quantitative) diagnostic products, but also, as its unique methodological contribution, mutually consistent diagnostic products.

Section S2: Microarchitecture of the ‘prompt auditor’
S2.1. Introduction
This section outlines the reference microarchitecture for the ‘prompt auditor’ archetype, describing its intended functional roles, information flow, and interaction patterns within the verification pipeline. The aim is to make explicit the conceptual structure by which the system enforces organized skepticism and multi-tier quality control. 
S2.2. Tier I: Audit of the evidence base and logical consistency
The first and most fundamental tier of the audit performs the function of verifying the empirical and logical foundation upon which the entire analytical structure of the ‘researcher’ is built. This stage, carried out by a specialized ‘data auditor agent’, is a direct analogue of primary peer review, where conclusions are checked for consistency with source data and the internal logic of the argumentation. The process is based on Stephen Toulmin’s model of argumentation, which evaluates the quality of the ‘grounds’ (empirical data) and the ‘warrant’ (the logical bridge connecting the data to the claim) (Toulmin, 1958). The task is broken down into three sequential checks.
●	S2.2.1. Evidence base quality audit. The agent-auditor receives both the user’s source text and the ‘diagnostic markup package’ (a structured set of quotes) generated by the ‘researcher’. Its task is to conduct a detailed semantic comparison to ensure the completeness (no significant omissions) and accuracy (no irrelevant inclusions) of the extracted evidence.
●	S2.2.2. Argumentation quality audit. The agent analyzes the differential diagnosis report to verify whether the arguments “for” and “against” each hypothesis are a logical consequence of the previously conducted aspect analyses. This step prevents the inclusion of “hallucinatory” or irrelevant statements in the argumentation.
●	S2.2.3. Falsification integrity audit. The agent purposefully analyzes the “Arguments AGAINST” block for the winning hypothesis to assess whether an honest attempt was made to find real weaknesses in the best model, or whether it was a formal, superficial criticism. This mechanism, directly implementing Popper’s principle of falsifiability (Popper, 1959), is key to counteracting confirmation bias.
S2.3. Tier II: Methodological congruence audit
The next tier of the audit, implemented by the ‘methodology audit agent’, performs a “semantic seam” check-it verifies the integrity of the transition from the analytical arbitration stage to the final narrative synthesis stage. If tier I checked the “data → argument” link, tier II checks the “argument → conclusion” link. Its task is to ensure that no semantic drift, distortion, or loss of key analytical findings occurred during the stylistic processing and structuring of the final report. This mechanism is an automated implementation of the “audit trail,” a key criterion for trustworthiness in qualitative research (Lincoln & Guba, 1985).
●	S2.3.1. Procedural congruence audit. The agent-auditor receives both the differential diagnosis report, which records the “winning” hypothesis, and the final qualitative conclusion. Its task is to conduct a detailed semantic comparison, verifying that the final text is an accurate, complete, and undistorted presentation of the hypothesis that won the competitive comparison.
●	S2.3.2. Theoretical congruence audit. The agent compares the statements in the final conclusion with the methodological passport of the construct. Its task is to verify that the interpretations given in the conclusion do not go beyond the accepted theoretical model on which the measuring instrument is based. This step is a direct implementation of the construct validity check (Cronbach & Meehl, 1955) at the final stage.
S2.4. Tier III: Quantitative meta-audit and objectification of reliability
The third echelon of the audit performs a meta-analytical and integrating function. It translates the series of qualitative, expert judgments made at the previous echelons into a single, objectified quantitative metric-the confidence score. This process, implemented by a specialized ‘calculator agent’, is key to ensuring the transparency and comparability of reliability assessments. Its task is not to trust the ‘researcher’s’ self-assessment, but to perform an independent, deterministic recalculation based on a formalized protocol, grounded in the methodology of constructing composite indices (OECD, 2008). The process involves aggregating and normalizing the audit findings from tiers I and II, applying a strict, predefined weighting formula, and generating a detailed textual justification that “reveals” the logic of the calculation.
S2.5. Tier IV: Final arbitration and flow management
The final tier of the audit performs a purely executive, rather than analytical, function. This stage, implemented by the ‘agent-arbitrator’, is the decision-making point in the entire verification pipeline. Having collected all the qualitative and quantitative audit findings from the previous echelons, it delivers a final, binding verdict (“Approved” or “Revision_Required”) and, if necessary, forms an “executive list”-a package with instructions for further work. This mechanism is the implementation of the “control layer” in hierarchical systems, which is separate from the analytical layers (Mesarović, Macko, & Takahara, 1970). The agent’s prompt contains rule-based logic that determines the final verdict based on a combination of all received audit assessments. If revision is required, the agent aggregates all recommendations from previous auditor reports and packages them into a single, structured JSON object that the orchestrator can directly pass back to the ‘researcher’ to start the correction cycle.

Section S3: Microarchitecture of the ‘prompt communicator’
S3.1. Introduction
This section outlines the reference microarchitecture for the ‘prompt communicator’ archetype, describing its intended functional roles, information flow, and interaction patterns in the communication and adaptation pipeline. Its purpose is to clarify, at a conceptual and analytical level, how expert-facing material is systematically restructured into user-facing outputs.
S3.2. Sub-pipeline I: Structural validation as a fault-tolerance mechanism
The first step in the communicator pipeline is not creative adaptation, but rigorous engineering verification. This sub-pipeline, implemented by the ‘structure auditor agent’, performs the function of input quality control. Its task is to receive the professional report and compare its structure with the reference “report plan” stored in the system’s configuration. The agent checks whether all the planned sections are present in the text and whether they contain any system “placeholders” for errors that occurred at previous stages.
This seemingly technical step has fundamental methodological significance. It is the implementation of the “fail-fast” principle, a best practice in designing reliable systems (Nygard, 2018). When the system detects an irreparable error at the input (a structurally damaged report), it immediately adapts to it instead of continuing processing in a known incorrect state, which could lead to a cascading failure. This mechanism ensures the resilience of the entire subsequent, resource-intensive adaptation pipeline. Instead of “crashing” when encountering a missing section, the system receives an accurate “roadmap” (validation_manifest) at the earliest stage, which determines which sections are valid and subject to further processing. The microarchitecture of the prompt for this agent is a classic example of a ‘prompt auditor’: it is as deterministic and procedural as possible, using rule-based logic to generate a machine-readable validation report.
S3.3. Sub-pipeline II: Iterative stylistic adaptation and consistency assurance
At the core of the communication adaptation process is an iterative sub-pipeline that sequentially rewrites the text of the professional report section by section. This process is implemented by two types of ‘agent-adapters’ and is designed to solve one of the most difficult tasks in generating long texts: ensuring global stylistic coherence.
The process begins with an ‘initializer agent’, which takes the first valid section of the report and performs its initial, “deep” adaptation, setting the “gold standard” of style for the entire subsequent document. Then, an ‘iterative adapter agent’ enters a cycle. For each subsequent section, it receives not only a new fragment of professional text, but also the complete, already adapted text of all previous sections. This mechanism is the implementation of a technique that can be called “context-driven stylistic consistency.” The already adapted text serves as a “stylistic tuning fork” or a dynamic few-shot example. This approach is grounded in the psycholinguistic theory of priming, which shows that the processing of one stimulus influences the processing of a subsequent one, making the model’s stylistic choices more consistent.
This iterative, context-enriched approach is a key methodological innovation that overcomes the tendency of LLMs to “drift stylistically” in long generations. It is grounded in discursive linguistics, which views text not as a set of sentences, but as a holistic, coherent structure where each subsequent element is linked to the previous ones (van Dijk, 1997). The architecture of this sub-pipeline emulates the work process of an experienced human editor who constantly rereads what has already been written to maintain stylistic unity.
S3.4. Sub-pipeline III: Dynamic creation of auxiliary teaching materials
To increase the educational value and comprehensibility of the adapted report, the system implements a parallel sub-pipeline for creating a dynamic glossary. This process is broken down into two specialized, sequential tasks.
First, a ‘lexicographer agent’ performs an intelligent scan of the entire adapted report. Its task is not simply to find difficult words, but to identify highly specialized psychological terms that may be incomprehensible to a non-specialist. This agent’s prompt implements data extraction with semantic filtering: it contains instructions to distinguish terms relevant to the glossary (e.g., “coping strategies,” “emotional regulation”) from commonly used psychological words (“emotions,” “stress”) and from the names of the constructs themselves, which are already explained in the main text.
Second, an ‘agent-glossator’ receives this filtered list of terms as input and generates a simple, clear, and methodologically correct definition for each. This two-step approach is grounded in the principles of instructional design, particularly the concept of “scaffolding” (Wood, Bruner, & Ross, 1976). The glossary acts as such scaffolding, providing the user with the necessary support to understand complex material and facilitating the assimilation of new concepts, which is a key element in the zone of proximal development, according to L.S. Vygotsky’s theory (Vygotsky, 2005).
S3.5. Sub-pipeline IV: Final synthesis and assembly
The final stage in the communicator pipeline is the assembly of all pre-processed and generated components into a single, coherent adapted report. This process is decomposed into semantically complex and simple tasks to ensure maximum quality.
The process begins with two parallel ‘synthesis agents’, which create the adapted introduction and conclusion. Unlike the iterative adapters, these agents receive the full context of the entire already adapted body of the report as input. The ‘introduction synthesis agent’ uses this context to create a conceptual and emotional "framework" that manages user expectations and subtly announces the key themes identified in the analysis, making the introduction personalized. The ‘conclusion agent-synthesizer’ performs the final meta-synthesis, “translating” complex integrative conclusions from the professional report into the language of usefulness and self-reflection, formulating “growth points” and a motivating conclusion.
Once all four components (introduction, body, conclusion, and optional glossary) are ready, the final ‘agent-assembler’ comes into play. Its function is purely technical and deterministic; it does not generate new meaning but performs a concatenation operation. This decomposition into “smart” synthesizers and a “dumb” assembler is an implementation of the single responsibility principle (Martin, 2017), separating complex, intellectual tasks from simple, mechanical ones. This final sub-pipeline ensures that the final adapted report will be not only semantically and stylistically coherent, but also structurally flawless.

Section S4: Multi-level validation protocol
S4.1. Introduction
This section details the comprehensive, multi-level validation program designed for the empirical verification of the emulated expertise architecture. The blueprint presented in the main article is a theoretical-methodological project; its practical efficacy, validity, and reliability must be rigorously established through empirical research. This protocol outlines a phased approach that moves from controlled psychometric evaluation to real-world didactic assessment, ensuring that the system is not only accurate but also effective and safe for its intended educational context. The protocol is designed to assess the system at three distinct levels: as a psychometric instrument, as a pedagogical tool, and as a technical system resistant to bias.
S4.2. Level 1: Psychometric validation
●	Objective: To assess the fundamental measurement properties of the system-its reliability and validity-in accordance with established standards for educational and psychological testing (AERA, APA, & NCME, 2014; Baturin et al., 2015).
●	Procedure:
1.	Data collection: A sample of participants will be recruited. Each participant will complete two sets of tasks:
▪	Text generation task: Participants will write a series of narrative essays in response to open-ended prompts designed to elicit behaviors and cognitions relevant to the target psychological constructs.
▪	Standardized testing battery: Participants will complete the full battery of “gold standard” introspective questionnaires that the system’s diagnostic modules are designed to emulate.
2.	Data analysis: The collected narrative essays will be processed by the system to generate both qualitative reports and quantitative scores for each construct. These outputs will then be compared against external criteria.
●	Key metrics and analyses:
o	Inter-rater reliability (for qualitative reports): A randomly selected subset of the qualitative reports generated by the system will be compared against reports generated independently by a panel of trained human experts analyzing the same essays. Agreement will be measured using appropriate statistical indices (e.g., Cohen’s kappa for categorical judgments, Krippendorff’s alpha for coded content), with a target α > 0.75. This will establish the system’s reliability in emulating expert judgment.
o	Convergent and discriminant validity (for quantitative scores): The quantitative scores generated by the system will be correlated with the scores from the standardized self-report questionnaires.
▪	Convergent validity: Strong, positive correlations are expected between the system’s score for a given construct and the score from the corresponding “gold standard” questionnaire.
▪	Discriminant validity: Weaker correlations are expected between the system’s score for a construct and scores from questionnaires measuring theoretically unrelated constructs. This will be analyzed using a multitrait-multimethod (MTMM) matrix.
o	Criterion-related validity: The system’s scores and qualitative findings will be correlated with external, objective criteria, such as academic performance (GPA), peer ratings of personality, or performance on behavioral tasks, to assess the system’s predictive power for real-world outcomes.
S4.3. Level 2: Didactic validation
●	Objective: To evaluate the pedagogical effectiveness and practical utility of the system’s outputs (specifically, the user-facing ‘adaptive report’) in a real-world educational setting. The focus is on measuring the system’s impact on student learning, metacognition, and self-reflection.
●	Procedure: A longitudinal randomized controlled trial (RCT) will be conducted over the course of an academic semester (Shadish, Cook, & Campbell, 2002). Student participants will be randomly assigned to one of two groups:
o	Experimental group: Students will complete narrative tasks at several points during the semester and receive the system-generated ‘adaptive report’ as formative feedback.
o	Control group: Students will complete the same narrative tasks but will receive no feedback or a placebo feedback (e.g., a generic summary of their writing style).
●	Key metrics and analyses:
o	Metacognitive skill development: Pre- and post-intervention measures of metacognitive awareness and self-regulation strategies will be administered to both groups. The primary hypothesis is that the experimental group will show significantly greater gains in metacognitive skills.
o	Development of ‘feedback literacy’: Qualitative analysis (interviews, focus groups) will be conducted to assess changes in students’ ability to understand, process, and act upon feedback, in line with the concept of ‘feedback literacy’ (Carless & Boud, 2018).
o	Academic performance and engagement: Course grades, assignment quality, and measures of student engagement will be compared between the two groups to assess the broader impact of the intervention.
o	Catalytic validity: Qualitative analysis will focus on the system’s ability to initiate meaningful self-reflection and dialogue (Lather, 1986). We will analyze how students use the report’s insights in discussions with peers, instructors, or academic advisors.
S4.4. Level 3: Bias and robustness audit
●	Objective: To systematically identify, quantify, and mitigate potential algorithmic biases within the system. This audit is designed to ensure that the system’s outputs are fair and equitable across different demographic groups.
●	Procedure: A series of stress tests and sensitivity analyses will be conducted. This involves creating a curated dataset of texts where demographic markers (e.g., related to gender, ethnicity, socioeconomic background) are either naturally present or synthetically inserted. These texts will then be processed by the system.
●	Key metrics and analyses:
o	Differential output analysis: The system’s outputs (both qualitative reports and quantitative scores) will be statistically analyzed to detect any significant, systematic differences that correlate with the demographic markers in the input texts. For example, we will test whether texts synthetically associated with a particular gender receive systematically different scores on constructs like “assertiveness” or “emotionality.”
o	Counterfactual fairness testing: For a given text, we will create a "counterfactual" version by changing only the demographic markers (e.g., changing names or pronouns). The system’s outputs for the original and counterfactual texts will be compared. A fair system should produce identical or near-identical outputs.
o	Adversarial testing: The system will be tested against inputs designed to exploit potential vulnerabilities, such as texts containing stereotypes or culturally specific idioms, to assess its robustness and ensure it does not rely on superficial or biased linguistic cues. Any identified biases will be addressed through targeted refinement of the analytical protocols and prompt engineering within the system’s agents.

Section S5: Epistemological and ontological foundations
S5.1. Introduction
This section elaborates on the deeper philosophical assumptions that underpin the emulated expertise architecture. The system’s design is not merely a technical solution but an engineered embodiment of a specific philosophical stance regarding the nature of the psyche (ontology) and the process of knowing it (epistemology). Articulating these foundations is crucial for distinguishing this approach from naive applications of AI that treat personality as a simple object to be measured, and for clarifying the true nature and limitations of the knowledge the system produces.
S5.2. Process-relational ontology: from static ‘traits’ to dynamic ‘patterns’
The system’s methodology implicitly rejects a “substance metaphysics” or “entity ontology” of the psyche-the common-sense view that personality traits are static, discrete "things" that exist inside a person. Such a view, which underpins much of classical psychometrics, struggles to account for the contextual variability and dynamic nature of human behavior.
Instead, the architecture is grounded in a process-relational ontology. This perspective posits that the “psychical” does not exist as a stable substance but emerges as a dynamic pattern of interaction between an individual and their environment. A personality trait, from this viewpoint, is not a fixed internal property but a probabilistic distribution of behavioral and cognitive states that becomes actualized in specific contexts.
●	Implementation in the system: This ontological commitment is directly implemented by the system’s focus on analyzing narratives of behavior-in-context. The system does not attempt to measure a static “thing” called “risk-readiness.” Instead, it models the process by which an individual construes, emotionally responds to, and formulates behavioral intentions regarding situations of risk as described in their own life stories. The output is not a measurement of a static entity, but a reconstruction of a dynamic, context-dependent cognitive-affective pattern.
●	Theoretical alignment: This stance aligns with the modern interactionist consensus in personality psychology (Funder, 2001), which holds that behavior is a function of the continuous interaction between the person and the situation. It also resonates deeply with the domestic tradition of the activity approach (Leont’ev, 1975), which views personality as being constituted and expressed through objective, goal-directed activity.
S5.3. Epistemology of critical fallibilism: knowledge as conjectures and refutations
The system’s approach to knowledge generation rejects naive empiricism or inductivism-the idea that knowledge is passively “extracted” or “induced” from data. Such a view is particularly problematic for LLMs, which are prone to identifying spurious correlations and presenting them as facts.
The architecture is instead an engineered embodiment of an epistemology of critical fallibilism, primarily derived from the philosophy of Karl Popper (Popper, 1959). This view holds that scientific knowledge is not derived from data but is constructed through an iterative and adversarial cycle of bold conjectures (creative, falsifiable hypotheses) and rigorous attempts at refutation (systematic efforts to prove them wrong). Knowledge is that which survives the most stringent attempts at falsification.
●	Implementation in the system: The multi-agent pipeline of the ‘domain expert’ unit is a direct implementation of this epistemological cycle:
1.	Conjecture: The ‘primary analytical synthesizer’ agent generates the initial, most plausible hypothesis based on the evidence. This is the bold conjecture.
2.	Refutation: The ‘skeptic-analyst’ agents are explicitly programmed for refutation. Their goal is to find anomalous data and construct the strongest possible competing hypotheses, representing systematic attempts to falsify the primary conjecture.
3.	Provisional survival: The ‘arbiter agent’ conducts a competitive comparison. The “winning” hypothesis is not considered "true," but rather the one that has best withstood criticism and provides the most comprehensive explanation for the available data at that moment.
●	Theoretical alignment: This process emulates the competition between “scientific research programmes” as described by Imre Lakatos (Lakatos, 1978), where theories compete based on their explanatory power and resilience to anomalies. By architecturally enforcing this adversarial process, the system moves beyond simple pattern recognition to a more rigorous form of knowledge construction.
S5.4. Hermeneutic status of the system: from ‘measurement’ to ‘standardized interpretation’
Given the ontological and epistemological foundations above, the system does not claim to be a “truth meter” that measures an objective, pre-existing reality of “personality.” Its claim to validity is of a different nature.
The system is best understood as a tool for standardized hermeneutics.
●	Hermeneutics: At its core, the system is engaged in a hermeneutic act-the process of interpretation, of making meaning from a complex and ambiguous text (the user’s narrative), as conceptualized by thinkers like Hans-Georg Gadamer (Gadamer, 1989).
●	Standardization: Unlike a human expert, whose interpretive process can be brilliant but is often implicit, idiosyncratic, and opaque, the system’s interpretive process is made explicit, transparent, and reproducible. The methodology (section S0), architectural mechanisms (sections S1-S3), and formal protocols (‘codebooks’) serve to standardize this hermeneutic act, ensuring that the same text will be interpreted through the same rigorous and verifiable procedure every time.
●	Nature of the output: The system’s output is therefore not “truth” but verifiable knowledge in the sense of Luciano Floridi (Floridi, 2011). It transforms unstructured, semantically ambiguous data (the text) into structured, well-formed, and meaningful information (the report) through an auditable process. Its ultimate claim to validity rests not on a correspondence to an objective reality, but on the coherence, rigor, and transparency of its interpretive methodology. The value of the system lies not in providing the “correct” answer, but in arriving at its answer through a methodologically sound, defensible, and fully auditable path.

Section S6: Full system blueprint and technical specifications
S6.1. Commitment to open science and reproducibility
The objective of this work is to make the scientific basis of the system inspectable. The high-level architecture, governing assumptions, methodological decisions, evaluation procedures, and observed behaviors are documented in a form intended to enable independent researchers to critically review the approach, assess its claims, and situate it within existing literature.
Consistent with this objective, the scope of disclosure in the present Supplementary Materials is limited to conceptual and methodological specification. Environment-specific engineering assets - including, but not limited to, full agent prompt templates, orchestration logic, deployment configurations, monitoring and audit procedures, runtime control policies, and other operational materials - are implementation-dependent and are not part of this Supplementary Materials.
The present document is the authoritative consolidation of supplementary methodological and architectural information for this work. 

Conclusion to supplementary materials
The sections of this Supplementary Materials document the system’s architectural rationale, design patterns, and evaluation methodology at a level intended to support critical examination of the claims in the main text. The objective is scientific clarity: to make the reasoning, high-level structures, and empirical procedures explicit enough to be interrogated, discussed, and compared.

References for supplementary materials
1.	AERA, APA, & NCME. (2014). Standards for educational and psychological testing. American Educational Research Association.
2.	Baturin, N. A., Vuchetich, E. V., Kostromina, S. N., Kukarkin, B. A., Kupriyanov, E. A., Lurie, E. V., Miting, O. V., Naumenko, A. S., Orel, E. A., & Poletaeva, Yu. S. (2015). Rossijskij standart testirovanija personala [Russian personnel testing standard]. Psihologija. Zhurnal Vysshej shkoly jekonomiki [Psychology. Journal of the Higher School of Economics], 12(2), 67–137.
3.	Carless, D., & Boud, D. (2018). The development of student feedback literacy: Enabling uptake of feedback. Assessment & Evaluation in Higher Education, 43(8), 1315–1325. https://doi.org/10.1080/02602938.2018.1463354
4.	Chamberlin, T. C. (1890). The method of multiple working hypotheses. Science, 15(366), 92–96.
5.	Craik, F. I. M., & Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11(6), 671–684. https://doi.org/10.1016/S0022-5371(72)80001-X
6.	Creswell, J. W., & Plano Clark, V. L. (2017). Designing and conducting mixed methods research (3rd ed.). SAGE.
7.	Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957
8.	Flavell, J. H. (1979). Metacognition and cognitive monitoring: A new area of cognitive–developmental inquiry. American Psychologist, 34(10), 906–911. https://doi.org/10.1037/0003-066X.34.10.906
9.	Floridi, L. (2011). The philosophy of information. Oxford University Press.
10.	Funder, D. C. (2001). Personality. Annual Review of Psychology, 52, 197–221. https://doi.org/10.1146/annurev.psych.52.1.197
11.	Gadamer, H.-G. (1989). Truth and method (2nd rev. ed.). Sheed & Ward.
12.	Lakatos, I. (1978). The methodology of scientific research programmes. Cambridge University Press.
13.	Lather, P. (1986). Issues of validity in openly ideological research. Interchange, 17(4), 63–84.
14.	Leont’ev, A. N. (1975). Dejatel’nost’. Soznanie. Lichnost’ [Activity. Consciousness. Personality]. Politizdat.
15.	Lincoln, Y. S., & Guba, E. G. (1985). Naturalistic inquiry. SAGE.
16.	Martin, R. C. (2017). Clean architecture: A craftsman’s guide to software structure and design. Pearson.
17.	Mesarović, M. D., Macko, D., & Takahara, Y. (1970). Theory of hierarchical, multilevel, systems. Academic Press.
18.	Nygard, M. T. (2018). Release it!: Design and deploy production-ready software (2nd ed.). Pragmatic Bookshelf.
19.	OECD, & JRC. (2008). Handbook on constructing composite indicators: Methodology and user guide. OECD Publishing. https://doi.org/10.1787/9789264043466-en
20.	Popper, K. (1959). The logic of scientific discovery. Hutchinson.
21.	Reichenbach, H. (1938). Experience and prediction: An analysis of the foundations and the structure of knowledge. University of Chicago Press.
22.	Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.
23.	Toulmin, S. E. (1958). The uses of argument. Cambridge University Press.
24.	van Dijk, T. A. (Ed.). (1997). Discourse as structure and process (Vol. 1). SAGE.
25.	Vygotsky, L. S. (2005). Psihologija razvitija cheloveka [The psychology of human development]. Smysl; Eksmo.
26.	Wood, D., Bruner, J. S., & Ross, G. (1976). The role of tutoring in problem solving. Journal of Child Psychology and Psychiatry, 17(2), 89–100. https://doi.org/10.1111/j.1469-7610.1976.tb00381.x

